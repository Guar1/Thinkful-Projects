{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RBM image classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Yghs2j5VpwfK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import cv2\n",
        "from random import shuffle\n",
        "import time\n",
        "\n",
        "from sklearn import linear_model, metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "# Import various componenets for model building\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import LSTM, Input, TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# Import the backend\n",
        "from keras import backend as K\n",
        "from tensorboardcolab import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "77rwlLRUsqxo",
        "colab_type": "code",
        "outputId": "9e978a4f-8d4a-41bf-ab56-e73d0bb61fce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EZawE4PTpwfT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_train():\n",
        "    normal_data=[]\n",
        "    pneumonia_data=[]\n",
        "    for img in tqdm(os.listdir(normal_train)):\n",
        "        path = os.path.join(normal_train, img)\n",
        "        img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (28,28))\n",
        "        img = img.astype(np.float32)/255.\n",
        "        normal_data.append(np.array(img))\n",
        "    for img in tqdm(os.listdir(pneumonia_train)):\n",
        "        path = os.path.join(pneumonia_train, img)\n",
        "        img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (28,28))\n",
        "        img = img.astype(np.float32)/255.\n",
        "        pneumonia_data.append(np.array(img))\n",
        "    training_data = normal_data + pneumonia_data\n",
        "    np.save('train_datanew.npy', training_data)\n",
        "    return training_data\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJ2Lmfy0pwfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = np.load('/content/drive/My Drive/Datasets/train_data.npy')\n",
        "labels = np.load('/content/drive/My Drive/Datasets/labels.npy')\n",
        "train_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LBLnRAiGpwfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = train_data\n",
        "Y = labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8lFhr7L2pwfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = X.reshape(5216,50,50, 1).astype('float32')/255\n",
        "Y = labels\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f6O9Q6tbUm0y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, Y = shuffle(X, Y, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gCwwdOyDbRMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3644bc63-fb31-4c22-81f0-8f180dbeaf50"
      },
      "cell_type": "code",
      "source": [
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://3457dd4e.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9P1qGOCQjwX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kMTDGQryv84g",
        "colab_type": "code",
        "outputId": "b9049716-19f3-4be5-b05a-c43b470ccf01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7852
        }
      },
      "cell_type": "code",
      "source": [
        "denselayers = [0,1,2]\n",
        "nodes = [32,64,128]\n",
        "convlayers = [2,3,4]\n",
        "\n",
        "for denselayer in denselayers:\n",
        "  for node in nodes:\n",
        "    for convlayer in convlayers:\n",
        "      \n",
        "      NAME = '{}-conv-{}-nodes-{}-dense{}'.format(convlayer, node, denselayer, int(time.time()))\n",
        "      print(NAME)\n",
        "      model = Sequential()\n",
        "      model.add(Conv2D(node,(3,3), activation='relu',  input_shape=(50,50,1)))\n",
        "      model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "      \n",
        "      for l in range(convlayer-1):\n",
        "        model.add(Conv2D(node,(3,3), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "      \n",
        "      model.add(Flatten())\n",
        "      for l in range(denselayer):\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "      \n",
        "      model.add(Dense(1, activation='sigmoid'))\n",
        "      model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "      \n",
        "      model.fit(X, Y, batch_size=16, epochs=10, verbose=1,validation_split=.1, callbacks=[TensorBoardColabCallback(tbc)])\n",
        "      \n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2-conv-32-nodes-0-dense1547104445\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 16s 3ms/step - loss: 0.3171 - acc: 0.8598 - val_loss: 0.1303 - val_acc: 0.9540\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.1294 - acc: 0.9506 - val_loss: 0.1757 - val_acc: 0.9195\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.1060 - acc: 0.9599 - val_loss: 0.1073 - val_acc: 0.9521\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.1003 - acc: 0.9599 - val_loss: 0.0925 - val_acc: 0.9674\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.0897 - acc: 0.9674 - val_loss: 0.0939 - val_acc: 0.9655\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.0890 - acc: 0.9636 - val_loss: 0.0842 - val_acc: 0.9732\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.0754 - acc: 0.9721 - val_loss: 0.0911 - val_acc: 0.9713\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.0704 - acc: 0.9732 - val_loss: 0.0931 - val_acc: 0.9713\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.0690 - acc: 0.9742 - val_loss: 0.0992 - val_acc: 0.9598\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 15s 3ms/step - loss: 0.0695 - acc: 0.9755 - val_loss: 0.0867 - val_acc: 0.9655\n",
            "3-conv-32-nodes-0-dense1547104596\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.3933 - acc: 0.8228 - val_loss: 0.2188 - val_acc: 0.8946\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1984 - acc: 0.9180 - val_loss: 0.1870 - val_acc: 0.9157\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1601 - acc: 0.9354 - val_loss: 0.1429 - val_acc: 0.9425\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1410 - acc: 0.9427 - val_loss: 0.1340 - val_acc: 0.9502\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1256 - acc: 0.9521 - val_loss: 0.1203 - val_acc: 0.9502\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1142 - acc: 0.9578 - val_loss: 0.1136 - val_acc: 0.9502\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1048 - acc: 0.9610 - val_loss: 0.1098 - val_acc: 0.9502\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1007 - acc: 0.9614 - val_loss: 0.1908 - val_acc: 0.9176\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 16s 3ms/step - loss: 0.0910 - acc: 0.9670 - val_loss: 0.1209 - val_acc: 0.9444\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.0888 - acc: 0.9680 - val_loss: 0.1079 - val_acc: 0.9483\n",
            "4-conv-32-nodes-0-dense1547104768\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.4453 - acc: 0.7874 - val_loss: 0.3062 - val_acc: 0.8487\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.2538 - acc: 0.8890 - val_loss: 0.1999 - val_acc: 0.9253\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1858 - acc: 0.9222 - val_loss: 0.1402 - val_acc: 0.9349\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1404 - acc: 0.9474 - val_loss: 0.1206 - val_acc: 0.9540\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1158 - acc: 0.9604 - val_loss: 0.1003 - val_acc: 0.9598\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0965 - acc: 0.9617 - val_loss: 0.0939 - val_acc: 0.9579\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0870 - acc: 0.9668 - val_loss: 0.0939 - val_acc: 0.9598\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0773 - acc: 0.9721 - val_loss: 0.0963 - val_acc: 0.9540\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0696 - acc: 0.9736 - val_loss: 0.1023 - val_acc: 0.9483\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0597 - acc: 0.9774 - val_loss: 0.0883 - val_acc: 0.9674\n",
            "2-conv-64-nodes-0-dense1547104944\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.2631 - acc: 0.8875 - val_loss: 0.1480 - val_acc: 0.9540\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.1284 - acc: 0.9491 - val_loss: 0.1151 - val_acc: 0.9540\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.1075 - acc: 0.9585 - val_loss: 0.1011 - val_acc: 0.9579\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.1051 - acc: 0.9599 - val_loss: 0.1060 - val_acc: 0.9579\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.0845 - acc: 0.9670 - val_loss: 0.1010 - val_acc: 0.9617\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.0711 - acc: 0.9725 - val_loss: 0.0938 - val_acc: 0.9674\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.0683 - acc: 0.9732 - val_loss: 0.0985 - val_acc: 0.9693\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.0684 - acc: 0.9759 - val_loss: 0.0987 - val_acc: 0.9732\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.0584 - acc: 0.9776 - val_loss: 0.0932 - val_acc: 0.9674\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 30s 6ms/step - loss: 0.0540 - acc: 0.9819 - val_loss: 0.0863 - val_acc: 0.9713\n",
            "3-conv-64-nodes-0-dense1547105245\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.3526 - acc: 0.8455 - val_loss: 0.1760 - val_acc: 0.9272\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.1480 - acc: 0.9418 - val_loss: 0.1294 - val_acc: 0.9521\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 31s 7ms/step - loss: 0.1121 - acc: 0.9585 - val_loss: 0.1307 - val_acc: 0.9444\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0937 - acc: 0.9659 - val_loss: 0.1005 - val_acc: 0.9655\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 34s 7ms/step - loss: 0.0811 - acc: 0.9678 - val_loss: 0.0945 - val_acc: 0.9693\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0727 - acc: 0.9742 - val_loss: 0.1318 - val_acc: 0.9579\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0676 - acc: 0.9751 - val_loss: 0.1090 - val_acc: 0.9598\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0563 - acc: 0.9806 - val_loss: 0.0942 - val_acc: 0.9655\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0595 - acc: 0.9781 - val_loss: 0.1035 - val_acc: 0.9598\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0544 - acc: 0.9793 - val_loss: 0.1036 - val_acc: 0.9636\n",
            "4-conv-64-nodes-0-dense1547105578\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.4603 - acc: 0.7938 - val_loss: 0.3377 - val_acc: 0.8467\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.2038 - acc: 0.9159 - val_loss: 0.2463 - val_acc: 0.8927\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.1378 - acc: 0.9457 - val_loss: 0.1103 - val_acc: 0.9540\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 35s 7ms/step - loss: 0.1140 - acc: 0.9572 - val_loss: 0.1235 - val_acc: 0.9464\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0939 - acc: 0.9666 - val_loss: 0.0859 - val_acc: 0.9655\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0859 - acc: 0.9683 - val_loss: 0.0846 - val_acc: 0.9693\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0717 - acc: 0.9732 - val_loss: 0.0821 - val_acc: 0.9693\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0593 - acc: 0.9778 - val_loss: 0.0884 - val_acc: 0.9693\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0629 - acc: 0.9776 - val_loss: 0.0931 - val_acc: 0.9655\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0518 - acc: 0.9821 - val_loss: 0.0887 - val_acc: 0.9693\n",
            "2-conv-128-nodes-0-dense1547105914\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 71s 15ms/step - loss: 0.2330 - acc: 0.9014 - val_loss: 0.1080 - val_acc: 0.9559\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 72s 15ms/step - loss: 0.1171 - acc: 0.9557 - val_loss: 0.1179 - val_acc: 0.9483\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 70s 15ms/step - loss: 0.0962 - acc: 0.9659 - val_loss: 0.0988 - val_acc: 0.9655\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 71s 15ms/step - loss: 0.0820 - acc: 0.9678 - val_loss: 0.0855 - val_acc: 0.9674\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 70s 15ms/step - loss: 0.0774 - acc: 0.9732 - val_loss: 0.1466 - val_acc: 0.9483\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 71s 15ms/step - loss: 0.0705 - acc: 0.9732 - val_loss: 0.1041 - val_acc: 0.9579\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 69s 15ms/step - loss: 0.0704 - acc: 0.9729 - val_loss: 0.0763 - val_acc: 0.9693\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 69s 15ms/step - loss: 0.0576 - acc: 0.9800 - val_loss: 0.0776 - val_acc: 0.9713\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 70s 15ms/step - loss: 0.0548 - acc: 0.9793 - val_loss: 0.0820 - val_acc: 0.9770\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 69s 15ms/step - loss: 0.0463 - acc: 0.9838 - val_loss: 0.0813 - val_acc: 0.9770\n",
            "3-conv-128-nodes-0-dense1547106620\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.3129 - acc: 0.8607 - val_loss: 0.1942 - val_acc: 0.9291\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.1491 - acc: 0.9446 - val_loss: 0.1191 - val_acc: 0.9540\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 78s 17ms/step - loss: 0.1017 - acc: 0.9608 - val_loss: 0.1007 - val_acc: 0.9636\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.0890 - acc: 0.9653 - val_loss: 0.1103 - val_acc: 0.9636\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 78s 17ms/step - loss: 0.0799 - acc: 0.9697 - val_loss: 0.1437 - val_acc: 0.9406\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.0650 - acc: 0.9768 - val_loss: 0.0866 - val_acc: 0.9732\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.0534 - acc: 0.9806 - val_loss: 0.0956 - val_acc: 0.9713\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.0443 - acc: 0.9851 - val_loss: 0.1316 - val_acc: 0.9598\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 78s 17ms/step - loss: 0.0438 - acc: 0.9832 - val_loss: 0.1123 - val_acc: 0.9713\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.0335 - acc: 0.9881 - val_loss: 0.1068 - val_acc: 0.9655\n",
            "4-conv-128-nodes-0-dense1547107412\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.4341 - acc: 0.8017 - val_loss: 0.2097 - val_acc: 0.9061\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.2069 - acc: 0.9135 - val_loss: 0.1650 - val_acc: 0.9368\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.1352 - acc: 0.9459 - val_loss: 0.1135 - val_acc: 0.9674\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.1005 - acc: 0.9608 - val_loss: 0.1067 - val_acc: 0.9540\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.0838 - acc: 0.9672 - val_loss: 0.0962 - val_acc: 0.9598\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.0763 - acc: 0.9710 - val_loss: 0.0866 - val_acc: 0.9713\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.0641 - acc: 0.9770 - val_loss: 0.0897 - val_acc: 0.9751\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.0601 - acc: 0.9798 - val_loss: 0.0845 - val_acc: 0.9713\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 78s 17ms/step - loss: 0.0550 - acc: 0.9789 - val_loss: 0.0938 - val_acc: 0.9693\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.0463 - acc: 0.9834 - val_loss: 0.1002 - val_acc: 0.9693\n",
            "2-conv-32-nodes-1-dense1547108214\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 20s 4ms/step - loss: 0.2279 - acc: 0.9067 - val_loss: 0.1175 - val_acc: 0.9617\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 20s 4ms/step - loss: 0.1164 - acc: 0.9555 - val_loss: 0.0934 - val_acc: 0.9636\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 20s 4ms/step - loss: 0.0956 - acc: 0.9651 - val_loss: 0.0914 - val_acc: 0.9617\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 20s 4ms/step - loss: 0.0842 - acc: 0.9717 - val_loss: 0.1237 - val_acc: 0.9521\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 20s 4ms/step - loss: 0.0810 - acc: 0.9706 - val_loss: 0.0981 - val_acc: 0.9598\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 19s 4ms/step - loss: 0.0639 - acc: 0.9757 - val_loss: 0.0932 - val_acc: 0.9617\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 19s 4ms/step - loss: 0.0644 - acc: 0.9740 - val_loss: 0.0834 - val_acc: 0.9693\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 20s 4ms/step - loss: 0.0485 - acc: 0.9823 - val_loss: 0.0860 - val_acc: 0.9751\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 20s 4ms/step - loss: 0.0425 - acc: 0.9842 - val_loss: 0.0955 - val_acc: 0.9693\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 20s 4ms/step - loss: 0.0397 - acc: 0.9859 - val_loss: 0.0912 - val_acc: 0.9713\n",
            "3-conv-32-nodes-1-dense1547108414\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.3021 - acc: 0.8666 - val_loss: 0.1460 - val_acc: 0.9425\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1282 - acc: 0.9546 - val_loss: 0.1493 - val_acc: 0.9310\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0960 - acc: 0.9663 - val_loss: 0.1581 - val_acc: 0.9349\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0841 - acc: 0.9702 - val_loss: 0.0950 - val_acc: 0.9674\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0755 - acc: 0.9715 - val_loss: 0.1085 - val_acc: 0.9617\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0602 - acc: 0.9764 - val_loss: 0.0994 - val_acc: 0.9636\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0598 - acc: 0.9766 - val_loss: 0.0987 - val_acc: 0.9655\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0595 - acc: 0.9791 - val_loss: 0.1012 - val_acc: 0.9751\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0490 - acc: 0.9817 - val_loss: 0.0974 - val_acc: 0.9693\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0438 - acc: 0.9842 - val_loss: 0.0894 - val_acc: 0.9674\n",
            "4-conv-32-nodes-1-dense1547108588\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 16s 3ms/step - loss: 0.4991 - acc: 0.7755 - val_loss: 0.2904 - val_acc: 0.8602\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 16s 4ms/step - loss: 0.2033 - acc: 0.9197 - val_loss: 0.1839 - val_acc: 0.9138\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 16s 3ms/step - loss: 0.1507 - acc: 0.9427 - val_loss: 0.1215 - val_acc: 0.9579\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 16s 4ms/step - loss: 0.1287 - acc: 0.9533 - val_loss: 0.1120 - val_acc: 0.9617\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 16s 4ms/step - loss: 0.1061 - acc: 0.9604 - val_loss: 0.0959 - val_acc: 0.9617\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 16s 4ms/step - loss: 0.0869 - acc: 0.9683 - val_loss: 0.0937 - val_acc: 0.9713\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 16s 4ms/step - loss: 0.0872 - acc: 0.9680 - val_loss: 0.0837 - val_acc: 0.9732\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 16s 3ms/step - loss: 0.0705 - acc: 0.9736 - val_loss: 0.0737 - val_acc: 0.9732\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 16s 3ms/step - loss: 0.0643 - acc: 0.9772 - val_loss: 0.0836 - val_acc: 0.9751\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 16s 3ms/step - loss: 0.0641 - acc: 0.9761 - val_loss: 0.0747 - val_acc: 0.9732\n",
            "2-conv-64-nodes-1-dense1547108756\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 39s 8ms/step - loss: 0.2529 - acc: 0.8943 - val_loss: 0.1074 - val_acc: 0.9617\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 38s 8ms/step - loss: 0.1224 - acc: 0.9527 - val_loss: 0.1136 - val_acc: 0.9636\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 38s 8ms/step - loss: 0.0921 - acc: 0.9651 - val_loss: 0.0936 - val_acc: 0.9693\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 39s 8ms/step - loss: 0.0777 - acc: 0.9704 - val_loss: 0.0915 - val_acc: 0.9713\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 38s 8ms/step - loss: 0.0689 - acc: 0.9740 - val_loss: 0.1150 - val_acc: 0.9540\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 38s 8ms/step - loss: 0.0663 - acc: 0.9736 - val_loss: 0.1000 - val_acc: 0.9674\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 38s 8ms/step - loss: 0.0608 - acc: 0.9768 - val_loss: 0.0922 - val_acc: 0.9713\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 38s 8ms/step - loss: 0.0518 - acc: 0.9802 - val_loss: 0.1063 - val_acc: 0.9598\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 38s 8ms/step - loss: 0.0475 - acc: 0.9834 - val_loss: 0.1921 - val_acc: 0.9330\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 38s 8ms/step - loss: 0.0415 - acc: 0.9853 - val_loss: 0.0964 - val_acc: 0.9732\n",
            "3-conv-64-nodes-1-dense1547109143\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.3611 - acc: 0.8428 - val_loss: 0.1408 - val_acc: 0.9444\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.1336 - acc: 0.9478 - val_loss: 0.1270 - val_acc: 0.9464\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.1048 - acc: 0.9574 - val_loss: 0.0950 - val_acc: 0.9617\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0784 - acc: 0.9723 - val_loss: 0.0998 - val_acc: 0.9598\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0670 - acc: 0.9768 - val_loss: 0.1504 - val_acc: 0.9540\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.0645 - acc: 0.9744 - val_loss: 0.0843 - val_acc: 0.9693\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.0493 - acc: 0.9802 - val_loss: 0.0963 - val_acc: 0.9693\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0519 - acc: 0.9806 - val_loss: 0.0959 - val_acc: 0.9751\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0405 - acc: 0.9855 - val_loss: 0.1604 - val_acc: 0.9502\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0374 - acc: 0.9844 - val_loss: 0.0944 - val_acc: 0.9617\n",
            "4-conv-64-nodes-1-dense1547109475\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.4320 - acc: 0.8095 - val_loss: 0.2372 - val_acc: 0.9138\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.1866 - acc: 0.9297 - val_loss: 0.1193 - val_acc: 0.9559\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.1298 - acc: 0.9487 - val_loss: 0.1201 - val_acc: 0.9502\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.0910 - acc: 0.9631 - val_loss: 0.0917 - val_acc: 0.9693\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.0827 - acc: 0.9689 - val_loss: 0.0788 - val_acc: 0.9713\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.0648 - acc: 0.9753 - val_loss: 0.0784 - val_acc: 0.9789\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0560 - acc: 0.9791 - val_loss: 0.0900 - val_acc: 0.9713\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0509 - acc: 0.9798 - val_loss: 0.0847 - val_acc: 0.9732\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.0442 - acc: 0.9815 - val_loss: 0.0943 - val_acc: 0.9636\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 32s 7ms/step - loss: 0.0457 - acc: 0.9830 - val_loss: 0.0978 - val_acc: 0.9674\n",
            "2-conv-128-nodes-1-dense1547109803\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 90s 19ms/step - loss: 0.2788 - acc: 0.8820 - val_loss: 0.1212 - val_acc: 0.9444\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 88s 19ms/step - loss: 0.1327 - acc: 0.9506 - val_loss: 0.1020 - val_acc: 0.9617\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 89s 19ms/step - loss: 0.1084 - acc: 0.9570 - val_loss: 0.1389 - val_acc: 0.9444\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 87s 19ms/step - loss: 0.0960 - acc: 0.9648 - val_loss: 0.0921 - val_acc: 0.9713\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 87s 19ms/step - loss: 0.0778 - acc: 0.9678 - val_loss: 0.1050 - val_acc: 0.9693\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 87s 19ms/step - loss: 0.0765 - acc: 0.9700 - val_loss: 0.0958 - val_acc: 0.9617\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 87s 19ms/step - loss: 0.0618 - acc: 0.9770 - val_loss: 0.0913 - val_acc: 0.9617\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 87s 19ms/step - loss: 0.0671 - acc: 0.9744 - val_loss: 0.1206 - val_acc: 0.9713\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 87s 18ms/step - loss: 0.0530 - acc: 0.9793 - val_loss: 0.1266 - val_acc: 0.9559\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 88s 19ms/step - loss: 0.0454 - acc: 0.9821 - val_loss: 0.0876 - val_acc: 0.9674\n",
            "3-conv-128-nodes-1-dense1547110687\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.3073 - acc: 0.8592 - val_loss: 0.3680 - val_acc: 0.8563\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.1369 - acc: 0.9489 - val_loss: 0.1204 - val_acc: 0.9598\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.0972 - acc: 0.9636 - val_loss: 0.1030 - val_acc: 0.9636\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.0861 - acc: 0.9668 - val_loss: 0.1112 - val_acc: 0.9559\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.0640 - acc: 0.9781 - val_loss: 0.0877 - val_acc: 0.9693\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.0567 - acc: 0.9798 - val_loss: 0.0870 - val_acc: 0.9751\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 80s 17ms/step - loss: 0.0555 - acc: 0.9802 - val_loss: 0.1386 - val_acc: 0.9502\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.0502 - acc: 0.9823 - val_loss: 0.1073 - val_acc: 0.9713\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.0365 - acc: 0.9864 - val_loss: 0.1539 - val_acc: 0.9521\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 79s 17ms/step - loss: 0.0372 - acc: 0.9857 - val_loss: 0.0838 - val_acc: 0.9713\n",
            "4-conv-128-nodes-1-dense1547111494\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.3921 - acc: 0.8262 - val_loss: 0.2579 - val_acc: 0.9004\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 89s 19ms/step - loss: 0.1664 - acc: 0.9365 - val_loss: 0.1549 - val_acc: 0.9425\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 82s 17ms/step - loss: 0.1220 - acc: 0.9538 - val_loss: 0.3344 - val_acc: 0.8812\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 83s 18ms/step - loss: 0.1034 - acc: 0.9631 - val_loss: 0.1084 - val_acc: 0.9636\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.0785 - acc: 0.9736 - val_loss: 0.0955 - val_acc: 0.9655\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.0765 - acc: 0.9770 - val_loss: 0.0869 - val_acc: 0.9674\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 82s 17ms/step - loss: 0.0669 - acc: 0.9757 - val_loss: 0.1001 - val_acc: 0.9636\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 82s 18ms/step - loss: 0.0635 - acc: 0.9783 - val_loss: 0.0828 - val_acc: 0.9693\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 83s 18ms/step - loss: 0.0525 - acc: 0.9808 - val_loss: 0.0913 - val_acc: 0.9674\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 81s 17ms/step - loss: 0.0470 - acc: 0.9838 - val_loss: 0.1123 - val_acc: 0.9598\n",
            "2-conv-32-nodes-2-dense1547112323\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 22s 5ms/step - loss: 0.2521 - acc: 0.8980 - val_loss: 0.1083 - val_acc: 0.9617\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 21s 4ms/step - loss: 0.1129 - acc: 0.9578 - val_loss: 0.1006 - val_acc: 0.9693\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 21s 4ms/step - loss: 0.0886 - acc: 0.9629 - val_loss: 0.0931 - val_acc: 0.9636\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 21s 4ms/step - loss: 0.0769 - acc: 0.9715 - val_loss: 0.1172 - val_acc: 0.9540\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 21s 4ms/step - loss: 0.0683 - acc: 0.9749 - val_loss: 0.0909 - val_acc: 0.9713\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 21s 5ms/step - loss: 0.0581 - acc: 0.9795 - val_loss: 0.0984 - val_acc: 0.9713\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 21s 4ms/step - loss: 0.0449 - acc: 0.9823 - val_loss: 0.1461 - val_acc: 0.9502\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 21s 4ms/step - loss: 0.0418 - acc: 0.9838 - val_loss: 0.1170 - val_acc: 0.9617\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 21s 4ms/step - loss: 0.0416 - acc: 0.9853 - val_loss: 0.1583 - val_acc: 0.9540\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 21s 5ms/step - loss: 0.0313 - acc: 0.9879 - val_loss: 0.0966 - val_acc: 0.9732\n",
            "3-conv-32-nodes-2-dense1547112538\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.3170 - acc: 0.8566 - val_loss: 0.1600 - val_acc: 0.9368\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.1312 - acc: 0.9470 - val_loss: 0.1094 - val_acc: 0.9540\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.0968 - acc: 0.9640 - val_loss: 0.1372 - val_acc: 0.9559\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.0750 - acc: 0.9734 - val_loss: 0.0996 - val_acc: 0.9617\n",
            "Epoch 5/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.0727 - acc: 0.9740 - val_loss: 0.0902 - val_acc: 0.9693\n",
            "Epoch 6/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0596 - acc: 0.9774 - val_loss: 0.0716 - val_acc: 0.9751\n",
            "Epoch 7/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0538 - acc: 0.9819 - val_loss: 0.0897 - val_acc: 0.9617\n",
            "Epoch 8/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.0554 - acc: 0.9802 - val_loss: 0.1032 - val_acc: 0.9617\n",
            "Epoch 9/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0440 - acc: 0.9851 - val_loss: 0.0958 - val_acc: 0.9674\n",
            "Epoch 10/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.0330 - acc: 0.9883 - val_loss: 0.1049 - val_acc: 0.9713\n",
            "4-conv-32-nodes-2-dense1547112721\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.3779 - acc: 0.8306 - val_loss: 0.2246 - val_acc: 0.8985\n",
            "Epoch 2/10\n",
            "4694/4694 [==============================] - 18s 4ms/step - loss: 0.1425 - acc: 0.9442 - val_loss: 0.2082 - val_acc: 0.9080\n",
            "Epoch 3/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.1028 - acc: 0.9604 - val_loss: 0.1051 - val_acc: 0.9559\n",
            "Epoch 4/10\n",
            "4694/4694 [==============================] - 17s 4ms/step - loss: 0.0842 - acc: 0.9685 - val_loss: 0.0882 - val_acc: 0.9636\n",
            "Epoch 5/10\n",
            " 864/4694 [====>.........................] - ETA: 13s - loss: 0.0759 - acc: 0.9676Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oW-BBhTon-zX",
        "colab_type": "code",
        "outputId": "58fff376-c75b-4b10-acb7-4ae669d72c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/My Drive/Datasets/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/My Drive/Datasets/model.h5\")\n",
        "print(\"Saved model to drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: google-drive-ocamlfuse: command not found\n",
            "Saved model to drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AFEmEdjEv9gG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "save_model = open('/content/drive/My Drive/Datasets/model.pickle', 'wb')\n",
        "pickle.dump(model, save_model)\n",
        "save_model.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSvEGj_kv9pa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "957bcf09-305c-423c-bcb9-c63d225bcca0"
      },
      "cell_type": "code",
      "source": [
        "#4 Conv layers w/ 64 nodes and 1 dense layer run with 6 epochs had the best validation accuracy and lowest loss\n",
        "\n",
        "denselayers = [1]\n",
        "nodes = [64]\n",
        "convlayers = [4]\n",
        "\n",
        "for denselayer in denselayers:\n",
        "  for node in nodes:\n",
        "    for convlayer in convlayers:\n",
        "      \n",
        "      NAME = '{}-conv-{}-nodes-{}-dense{}'.format(convlayer, node, denselayer, int(time.time()))\n",
        "      print(NAME)\n",
        "      model = Sequential()\n",
        "      model.add(Conv2D(node,(3,3), activation='relu',  input_shape=(50,50,1)))\n",
        "      model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "      \n",
        "      for l in range(convlayer-1):\n",
        "        model.add(Conv2D(node,(3,3), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "      \n",
        "      model.add(Flatten())\n",
        "      for l in range(denselayer):\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "      \n",
        "      model.add(Dense(1, activation='sigmoid'))\n",
        "      model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "      \n",
        "      model.fit(X, Y, batch_size=16, epochs=6, verbose=1,validation_split=.1, callbacks=[TensorBoardColabCallback(tbc)])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4-conv-64-nodes-1-dense1547150712\n",
            "Train on 4694 samples, validate on 522 samples\n",
            "Epoch 1/6\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.4582 - acc: 0.7836 - val_loss: 0.3017 - val_acc: 0.8602\n",
            "Epoch 2/6\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.1901 - acc: 0.9244 - val_loss: 0.1416 - val_acc: 0.9425\n",
            "Epoch 3/6\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.1344 - acc: 0.9480 - val_loss: 0.1035 - val_acc: 0.9579\n",
            "Epoch 4/6\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0981 - acc: 0.9663 - val_loss: 0.0846 - val_acc: 0.9693\n",
            "Epoch 5/6\n",
            "4694/4694 [==============================] - 33s 7ms/step - loss: 0.0766 - acc: 0.9734 - val_loss: 0.0762 - val_acc: 0.9693\n",
            "Epoch 6/6\n",
            "4694/4694 [==============================] - 34s 7ms/step - loss: 0.0728 - acc: 0.9742 - val_loss: 0.0760 - val_acc: 0.9732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HbOWFOAEv9vs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testdata = np.load('/content/drive/My Drive/Datasets/xraytest (1).npy')\n",
        "testlabels = np.load('/content/drive/My Drive/Datasets/xraytestlabels.npy')\n",
        "testdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtwYETIGv9t9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#shuffle the data to avoid bias\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "Xtest, Ytest = shuffle(testdata, testlabels, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qS4f1Dubv9nK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xtest = Xtest.reshape(-1,50,50,1)/255\n",
        "Xtest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U8PCgXEGv9lj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "0d0ece65-67ba-44ec-b54f-6e9c7b3b5222"
      },
      "cell_type": "code",
      "source": [
        "predy = model.predict(Xtest)\n",
        "scores = model.evaluate(Xtest, Ytest)\n",
        "print('Loss', scores[0])\n",
        "print('Accuracy', scores[1])"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "624/624 [==============================] - 1s 2ms/step\n",
            "Loss 0.7385539229099567\n",
            "Accuracy 0.7964743589743589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VEdaYruQb1Bj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10833
        },
        "outputId": "d92daec5-bd25-4421-b377-e499d7079e41"
      },
      "cell_type": "code",
      "source": [
        "predy = np.round_(predy, decimals=0)\n",
        "predy"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "metadata": {
        "id": "rU0htGb8v9eH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "23dfa0ae-5c2e-47a9-abaa-6061952a9312"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "conf = confusion_matrix(Ytest, predy)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(conf, hide_ticks=True, cmap=plt.cm.BuPu)\n",
        "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
        "plt.show()\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdd315d1ef0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAFdCAYAAADcwmArAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu2M1/lTrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdd315d5470>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "PkPz4pcWv9cf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ff7ce4e3-49fa-46a1-fdab-4b5b71839333"
      },
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = conf.ravel()\n",
        "\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "\n",
        "print(\"Recall: {:.2f}\".format(recall))\n",
        "print(\"Precision: {:.2f}\".format(precision))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall: 0.98\n",
            "Precision: 0.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J2txBMmov9WG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "at7l4559v9UF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y12PPvqdv9Sp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmTA4WZ6v9Q6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6KLRn57ev9Ks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vy7z8v3yv9Ib",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ba1DxQ7v9G4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-bkTs-9sv9Bs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3e7EY7AUv8_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "anTugNh8v89g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRDQuSC_v87z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tsFW56Yqv82D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_vtx_2fpwgh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Addressing Class imbalance\n",
        "\n",
        "Here I resample the pneumonia class to be equal to the normal class."
      ]
    },
    {
      "metadata": {
        "id": "Lbmldfn-pwgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_trainUS():\n",
        "    normal_data=[]\n",
        "    pneumonia_data=[]\n",
        "    for img in tqdm(os.listdir(normal_train)):\n",
        "        path = os.path.join(normal_train, img)\n",
        "        img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (28,28))\n",
        "        img = img.astype(np.float32)/255.\n",
        "        normal_data.append(np.array(img))\n",
        "    for img in tqdm(os.listdir(pneumonia_train)):\n",
        "        path = os.path.join(pneumonia_train, img)\n",
        "        img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (28,28))\n",
        "        img = img.astype(np.float32)/255.\n",
        "        pneumonia_data.append(np.array(img))\n",
        "    shuffle(pneumonia_data)\n",
        "    training_data = normal_data + pneumonia_data[0:1341]\n",
        "    np.save('train_dataUS.npy', training_data)\n",
        "    return training_data\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2g62DAwPpwgk",
        "colab_type": "code",
        "outputId": "7b17af56-dc15-468b-ea58-995aa4d3a7eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "cell_type": "code",
      "source": [
        "training_Data = create_trainUS()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-96a63b165729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_Data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_trainUS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-3a7bd363e945>\u001b[0m in \u001b[0;36mcreate_trainUS\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnormal_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpneumonia_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'normal_train' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-xtyqkzgpwgo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_data = np.array(training_Data)\n",
        "training_data = training_data.reshape(2682, 28*28)\n",
        "\n",
        "USlabels = [0] *1341 + [1] *1341\n",
        "Y = np.array(USlabels)\n",
        "\n",
        "X = training_data\n",
        "X, Y = nudge_dataset(X,Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBif26mcpwgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "2umdTj8spwgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=10000,\n",
        "                                           multi_class='multinomial')\n",
        "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
        "\n",
        "rbm_features_classifier = Pipeline(\n",
        "    steps=[('rbm', rbm), ('logistic', logistic)])\n",
        "\n",
        "# Training\n",
        "rbm.learning_rate = 0.06\n",
        "rbm.n_iter = 20\n",
        "rbm.n_components = 100\n",
        "logistic.C = 6000\n",
        "\n",
        "# Training RBM-Logistic Pipeline\n",
        "rbm_features_classifier.fit(X_train, y_train)\n",
        "\n",
        "raw_pixel_classifier = clone(logistic)\n",
        "raw_pixel_classifier.C = 100.\n",
        "raw_pixel_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation\n",
        "Y_pred = rbm_features_classifier.predict(X_test)\n",
        "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
        "    metrics.classification_report(y_test, Y_pred)))\n",
        "\n",
        "Y_pred = raw_pixel_classifier.predict(X_test)\n",
        "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
        "    metrics.classification_report(y_test, Y_pred)))\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(4.2, 4))\n",
        "for i, comp in enumerate(rbm.components_):\n",
        "    plt.subplot(10, 10, i + 1)\n",
        "    plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r,\n",
        "               interpolation='nearest')\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "plt.suptitle('100 components extracted by RBM', fontsize=16)\n",
        "plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NwMQPTUpwgx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "Overall Logistic regression on the raw image files outperformed the RBM extracted images. Further tweaking of the RBM parameters such as increasing n_components may increase performance.\n",
        "\n",
        "There was a class imbalance ~ 3:1 pneunomia:normal that caused logistic regression on RBM features to only predict pneumonia. After correcting for the class imbalance by creating a random sample of pneumonia images the same length as normal images we saw a more accurate prediction. RBM features still performed worse than the raw image files (82% recall and precision vs 91%/93% precision/recall of raw images). \n",
        "\n",
        "Shifting the images to artificially create new training data is also a questionable practice. The shifted images should be the same as the original images and be weighted equally which should lead to the same performance, but instead we saw a significant increase in performance.\n",
        "\n",
        "Another factor to consider is the effect that image size may have on performance. a 28x28 image shifted 1 pixel will have a much greater difference than a 100x100 image shifted 1 pixel. Further exploration of image size should also be explored."
      ]
    },
    {
      "metadata": {
        "id": "ex8xdBlKpwgy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2spdQnN7sMri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ijgZsJZ7sMt5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkD63NNMsMyw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V7WNQZ10sM35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kSkaGotmsM8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mfn9I8QAsM1-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X6v3TiQysMwz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}